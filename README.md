# ç”¨DQNå¼ºåŒ–å­¦ä¹ ç®—æ³•ç©â€œåˆæˆå¤§è¥¿ç“œâ€ï¼

è§†é¢‘é“¾æ¥ï¼š

https://www.bilibili.com/video/BV1Tz4y1U7HE

https://www.bilibili.com/video/BV1Wy4y1n73E

https://www.bilibili.com/video/BV1gN411d7dr

## 1. å®‰è£…ä¾èµ–åº“

> å…¶ä¸­æ¸¸æˆä»£ç ä½¿ç”¨pygameé‡æ„

> ç‰©ç†æ¨¡å—ä½¿ç”¨pymunk

æ³¨ï¼špaddlepaddleç‰ˆæœ¬ä¸º1.8.0ï¼Œparlç‰ˆæœ¬ä¸º1.3.1


```
# !pip install pygame -i https://mirror.baidu.com/pypi/simple
# !pip install parl==1.3.1 -i https://mirror.baidu.com/pypi/simple
# !pip install pymunk
```


```
# !unzip work/code.zip -d ./
```

## 2. è®¾ç½®ç¯å¢ƒå˜é‡

> ç”±äºnotebookæ— æ³•æ˜¾ç¤ºpygameç•Œé¢ï¼Œæ‰€ä»¥æˆ‘ä»¬è®¾ç½®å¦‚ä¸‹ç¯å¢ƒå˜é‡


```
import os
os.putenv('SDL_VIDEODRIVER', 'fbcon')
os.environ["SDL_VIDEODRIVER"] = "dummy"
```

## 3. æ„å»ºå¤šå±‚ç¥ç»ç½‘ç»œ

> è¯¥ç‰ˆæœ¬ä½¿ç”¨ä¸¤å±‚å…¨è¿æ¥å±‚

> å·ç§¯ç¥ç»ç½‘ç»œç‰ˆæœ¬ä¸ºï¼šhttps://aistudio.baidu.com/aistudio/projectdetail/1540300


```
import parl
from parl import layers
import paddle.fluid as fluid
import copy
import numpy as np
import os
from parl.utils import logger
import random
import collections

LEARN_FREQ = 5 # è®­ç»ƒé¢‘ç‡ï¼Œä¸éœ€è¦æ¯ä¸€ä¸ªstepéƒ½learnï¼Œæ”’ä¸€äº›æ–°å¢ç»éªŒåå†learnï¼Œæé«˜æ•ˆç‡
MEMORY_SIZE = 20000    # replay memoryçš„å¤§å°ï¼Œè¶Šå¤§è¶Šå ç”¨å†…å­˜
MEMORY_WARMUP_SIZE = 200  # replay_memory é‡Œéœ€è¦é¢„å­˜ä¸€äº›ç»éªŒæ•°æ®ï¼Œå†å¼€å¯è®­ç»ƒ
BATCH_SIZE = 32   # æ¯æ¬¡ç»™agent learnçš„æ•°æ®æ•°é‡ï¼Œä»replay memoryéšæœºé‡Œsampleä¸€æ‰¹æ•°æ®å‡ºæ¥
LEARNING_RATE = 0.001 # å­¦ä¹ ç‡
GAMMA = 0.99 # reward çš„è¡°å‡å› å­ï¼Œä¸€èˆ¬å– 0.9 åˆ° 0.999 ä¸ç­‰


class Model(parl.Model):
    def __init__(self, act_dim):
        hid1_size = 256
        hid2_size = 256
        # 3å±‚å…¨è¿æ¥ç½‘ç»œ
        self.fc1 = layers.fc(size=hid1_size, act='relu')
        self.fc2 = layers.fc(size=hid2_size, act='relu')
        self.fc3 = layers.fc(size=act_dim, act=None)

    def value(self, obs):
        # å®šä¹‰ç½‘ç»œ
        # è¾“å…¥stateï¼Œè¾“å‡ºæ‰€æœ‰actionå¯¹åº”çš„Qï¼Œ[Q(s,a1), Q(s,a2), Q(s,a3)...]
        h1 = self.fc1(obs)
        h2 = self.fc2(h1)
        Q = self.fc3(h2)
        return Q
```


## 4. æ„å»ºDQNç®—æ³•ã€Agentå’Œç»éªŒæ± 


```
# from parl.algorithms import DQN # ä¹Ÿå¯ä»¥ç›´æ¥ä»parlåº“ä¸­å¯¼å…¥DQNç®—æ³•
import pygame

class DQN(parl.Algorithm):
    def __init__(self, model, act_dim=None, gamma=None, lr=None):
        """ DQN algorithm
        
        Args:
            model (parl.Model): å®šä¹‰Qå‡½æ•°çš„å‰å‘ç½‘ç»œç»“æ„
            act_dim (int): actionç©ºé—´çš„ç»´åº¦ï¼Œå³æœ‰å‡ ä¸ªaction
            gamma (float): rewardçš„è¡°å‡å› å­
            lr (float): learning rate å­¦ä¹ ç‡.
        """
        self.model = model
        self.target_model = copy.deepcopy(model)

        assert isinstance(act_dim, int)
        assert isinstance(gamma, float)
        assert isinstance(lr, float)
        self.act_dim = act_dim
        self.gamma = gamma
        self.lr = lr

    def predict(self, obs):
        """ ä½¿ç”¨self.modelçš„valueç½‘ç»œæ¥è·å– [Q(s,a1),Q(s,a2),...]
        """
        return self.model.value(obs)

    def learn(self, obs, action, reward, next_obs, terminal):
        """ ä½¿ç”¨DQNç®—æ³•æ›´æ–°self.modelçš„valueç½‘ç»œ
        """
        # ä»target_modelä¸­è·å– max Q' çš„å€¼ï¼Œç”¨äºè®¡ç®—target_Q
        next_pred_value = self.target_model.value(next_obs)
        best_v = layers.reduce_max(next_pred_value, dim=1)
        best_v.stop_gradient = True  # é˜»æ­¢æ¢¯åº¦ä¼ é€’
        terminal = layers.cast(terminal, dtype='float32')
        target = reward + (1.0 - terminal) * self.gamma * best_v

        pred_value = self.model.value(obs)  # è·å–Qé¢„æµ‹å€¼
        # å°†actionè½¬onehotå‘é‡ï¼Œæ¯”å¦‚ï¼š3 => [0,0,0,1,0]
        action_onehot = layers.one_hot(action, self.act_dim)
        action_onehot = layers.cast(action_onehot, dtype='float32')
        # ä¸‹é¢ä¸€è¡Œæ˜¯é€å…ƒç´ ç›¸ä¹˜ï¼Œæ‹¿åˆ°actionå¯¹åº”çš„ Q(s,a)
        # æ¯”å¦‚ï¼špred_value = [[2.3, 5.7, 1.2, 3.9, 1.4]], action_onehot = [[0,0,0,1,0]]
        #  ==> pred_action_value = [[3.9]]
        pred_action_value = layers.reduce_sum(
            layers.elementwise_mul(action_onehot, pred_value), dim=1)

        # è®¡ç®— Q(s,a) ä¸ target_Qçš„å‡æ–¹å·®ï¼Œå¾—åˆ°loss
        cost = layers.square_error_cost(pred_action_value, target)
        cost = layers.reduce_mean(cost)
        optimizer = fluid.optimizer.Adam(learning_rate=self.lr)  # ä½¿ç”¨Adamä¼˜åŒ–å™¨
        optimizer.minimize(cost)
        return cost

    def sync_target(self):
        """ æŠŠ self.model çš„æ¨¡å‹å‚æ•°å€¼åŒæ­¥åˆ° self.target_model
        """
        self.model.sync_weights_to(self.target_model)


class Agent(parl.Agent):
    def __init__(self,
                 algorithm,
                 obs_dim,
                 act_dim,
                 e_greed=0.1,
                 e_greed_decrement=0):
        assert isinstance(obs_dim, int)
        assert isinstance(act_dim, int)
        self.obs_dim = obs_dim
        self.act_dim = act_dim
        super(Agent, self).__init__(algorithm)

        self.global_step = 0
        self.update_target_steps = 200  # æ¯éš”200ä¸ªtraining stepså†æŠŠmodelçš„å‚æ•°å¤åˆ¶åˆ°target_modelä¸­

        self.e_greed = e_greed  # æœ‰ä¸€å®šæ¦‚ç‡éšæœºé€‰å–åŠ¨ä½œï¼Œæ¢ç´¢
        self.e_greed_decrement = e_greed_decrement  # éšç€è®­ç»ƒé€æ­¥æ”¶æ•›ï¼Œæ¢ç´¢çš„ç¨‹åº¦æ…¢æ…¢é™ä½

    def build_program(self):
        self.pred_program = fluid.Program()
        self.learn_program = fluid.Program()

        with fluid.program_guard(self.pred_program):  # æ­å»ºè®¡ç®—å›¾ç”¨äº é¢„æµ‹åŠ¨ä½œï¼Œå®šä¹‰è¾“å…¥è¾“å‡ºå˜é‡
            obs = layers.data(
                name='obs', shape=[self.obs_dim], dtype='float32')
            self.value = self.alg.predict(obs)

        with fluid.program_guard(self.learn_program):  # æ­å»ºè®¡ç®—å›¾ç”¨äº æ›´æ–°Qç½‘ç»œï¼Œå®šä¹‰è¾“å…¥è¾“å‡ºå˜é‡
            obs = layers.data(
                name='obs', shape=[self.obs_dim], dtype='float32')
            action = layers.data(name='act', shape=[1], dtype='int32')
            reward = layers.data(name='reward', shape=[], dtype='float32')
            next_obs = layers.data(
                name='next_obs', shape=[self.obs_dim], dtype='float32')
            terminal = layers.data(name='terminal', shape=[], dtype='bool')
            self.cost = self.alg.learn(obs, action, reward, next_obs, terminal)

    def sample(self, obs):
        sample = np.random.rand()  # äº§ç”Ÿ0~1ä¹‹é—´çš„å°æ•°
        if sample < self.e_greed:
            act = np.random.randint(self.act_dim)  # æ¢ç´¢ï¼šæ¯ä¸ªåŠ¨ä½œéƒ½æœ‰æ¦‚ç‡è¢«é€‰æ‹©
        else:
            act = self.predict(obs)  # é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
        self.e_greed = max(
            0.01, self.e_greed - self.e_greed_decrement)  # éšç€è®­ç»ƒé€æ­¥æ”¶æ•›ï¼Œæ¢ç´¢çš„ç¨‹åº¦æ…¢æ…¢é™ä½
        return act

    def predict(self, obs):  # é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
        obs = np.expand_dims(obs, axis=0)
        pred_Q = self.fluid_executor.run(
            self.pred_program,
            feed={'obs': obs.astype('float32')},
            fetch_list=[self.value])[0]
        pred_Q = np.squeeze(pred_Q, axis=0)
        act = np.argmax(pred_Q)  # é€‰æ‹©Qæœ€å¤§çš„ä¸‹æ ‡ï¼Œå³å¯¹åº”çš„åŠ¨ä½œ
        return act

    def learn(self, obs, act, reward, next_obs, terminal):
        # æ¯éš”200ä¸ªtraining stepsåŒæ­¥ä¸€æ¬¡modelå’Œtarget_modelçš„å‚æ•°
        if self.global_step % self.update_target_steps == 0:
            self.alg.sync_target()
        self.global_step += 1

        act = np.expand_dims(act, -1)
        feed = {
            'obs': obs.astype('float32'),
            'act': act.astype('int32'),
            'reward': reward,
            'next_obs': next_obs.astype('float32'),
            'terminal': terminal
        }
        cost = self.fluid_executor.run(
            self.learn_program, feed=feed, fetch_list=[self.cost])[0]  # è®­ç»ƒä¸€æ¬¡ç½‘ç»œ
        return cost



class ReplayMemory(object):
    def __init__(self, max_size):
        self.buffer = collections.deque(maxlen=max_size)

    # å¢åŠ ä¸€æ¡ç»éªŒåˆ°ç»éªŒæ± ä¸­
    def append(self, exp):
        self.buffer.append(exp)

    # ä»ç»éªŒæ± ä¸­é€‰å–Næ¡ç»éªŒå‡ºæ¥
    def sample(self, batch_size):
        mini_batch = random.sample(self.buffer, batch_size)
        obs_batch, action_batch, reward_batch, next_obs_batch, done_batch = [], [], [], [], []

        for experience in mini_batch:
            s, a, r, s_p, done = experience
            obs_batch.append(s)
            action_batch.append(a)
            reward_batch.append(r)
            next_obs_batch.append(s_p)
            done_batch.append(done)

        return np.array(obs_batch).astype('float32'), \
            np.array(action_batch).astype('float32'), np.array(reward_batch).astype('float32'),\
            np.array(next_obs_batch).astype('float32'), np.array(done_batch).astype('float32')

    def __len__(self):
        return len(self.buffer)


# è®­ç»ƒä¸€ä¸ªepisode
def run_episode(env, agent, rpm, episode):
    total_reward = 0
    env.reset()
    action = np.random.randint(0, env.action_num - 1)
    obs, _, _ = env.next(action)
    step = 0
    while True:
        step += 1
        action = agent.sample(obs)  # é‡‡æ ·åŠ¨ä½œï¼Œæ‰€æœ‰åŠ¨ä½œéƒ½æœ‰æ¦‚ç‡è¢«å°è¯•åˆ°
        next_obs, reward, done = env.next(action)
        rpm.append((obs, action, reward, next_obs, done))

        # train model
        if (len(rpm) > MEMORY_WARMUP_SIZE) and (step % LEARN_FREQ == 0):
            (batch_obs, batch_action, batch_reward, batch_next_obs,
             batch_done) = rpm.sample(BATCH_SIZE)
            train_loss = agent.learn(batch_obs, batch_action, batch_reward,
                                     batch_next_obs,
                                     batch_done)  # s,a,r,s',done

        total_reward += reward
        obs = next_obs
        if done:
            break
        if not step % 20:
            logger.info('step:{} e_greed:{} action:{} reward:{}'.format(
                step, agent.e_greed, action, reward))
        if not step % 500:
            image = pygame.surfarray.array3d(
                 pygame.display.get_surface()).copy()
            image = np.flip(image[:, :, [2, 1, 0]], 0)
            image = np.rot90(image, 3)
            img_pt = os.path.join('outputs', 'snapshoot_{}_{}.jpg'.format(episode, step))
            cv2.imwrite(img_pt, image)
    return total_reward
```

    pygame 2.0.1 (SDL 2.0.14, Python 3.7.4)
    Hello from the pygame community. https://www.pygame.org/contribute.html


## 5. åˆ›å»ºAgentå®ä¾‹


```
from State2NN import AI_Board

env = AI_Board()  
action_dim = env.action_num  
obs_shape = 16 * 13  
e_greed = 0.2

rpm = ReplayMemory(MEMORY_SIZE)  # DQNçš„ç»éªŒå›æ”¾æ± 

# æ ¹æ®parlæ¡†æ¶æ„å»ºagent
model = Model(act_dim=action_dim)
algorithm = DQN(model, act_dim=action_dim, gamma=GAMMA, lr=LEARNING_RATE)
agent = Agent(
    algorithm,
    obs_dim=obs_shape,
    act_dim=action_dim,
    e_greed=e_greed,  # æœ‰ä¸€å®šæ¦‚ç‡éšæœºé€‰å–åŠ¨ä½œï¼Œæ¢ç´¢
    e_greed_decrement=1e-6)  # éšç€è®­ç»ƒé€æ­¥æ”¶æ•›ï¼Œæ¢ç´¢çš„ç¨‹åº¦æ…¢æ…¢é™ä½
```

    [32m[02-20 22:45:25 MainThread @machine_info.py:86][0m nvidia-smi -L found gpu count: 1
    [32m[02-20 22:45:25 MainThread @machine_info.py:86][0m nvidia-smi -L found gpu count: 1


## 6. è®­ç»ƒæ¨¡å‹


```
from State2NN import AI_Board
import os

dirs = ['weights', 'outputs']
for d in dirs:
    if not os.path.exists(d):
        os.mkdir(d)

# åŠ è½½æ¨¡å‹
# save_path = './dqn_model.ckpt'
# agent.restore(save_path)

# å…ˆå¾€ç»éªŒæ± é‡Œå­˜ä¸€äº›æ•°æ®ï¼Œé¿å…æœ€å¼€å§‹è®­ç»ƒçš„æ—¶å€™æ ·æœ¬ä¸°å¯Œåº¦ä¸å¤Ÿ
while len(rpm) < MEMORY_WARMUP_SIZE:
    run_episode(env, agent, rpm, episode=0)

max_episode = 2000

# å¼€å§‹è®­ç»ƒ
episode = 0
while episode < max_episode:  # è®­ç»ƒmax_episodeä¸ªå›åˆï¼Œtestéƒ¨åˆ†ä¸è®¡ç®—å…¥episodeæ•°é‡
    # train part
    for i in range(0, 50):
        total_reward = run_episode(env, agent, rpm, episode+1)
        episode += 1
        save_path = './weights/dqn_model_episode_{}.ckpt'.format(episode)
        agent.save(save_path)
        print('-[INFO] episode:{}, model saved at {}'.format(episode, save_path))
        env.reset()

# è®­ç»ƒç»“æŸï¼Œä¿å­˜æ¨¡å‹
save_path = './final.ckpt'
agent.save(save_path)
```

## 7. æ¸¸æˆç¯å¢ƒè¡¥å……è¯´æ˜



### 7.1 æ¸¸æˆå…±æœ‰11ç§æ°´æœï¼š

![](https://ai-studio-static-online.cdn.bcebos.com/6758c92fb70e4b59904cd0d39b7e8b4e9c70943c4f9046129f289c8e8b52ecf5)

### 7.2 ç¢°æ’æ£€æµ‹ï¼š


```python
def setup_collision_handler(self):
        def post_solve_bird_line(arbiter, space, data):
            if not self.lock:
                self.lock = True
                b1, b2 = None, None
                i = arbiter.shapes[0].collision_type + 1
                x1, y1 = arbiter.shapes[0].body.position
                x2, y2 = arbiter.shapes[1].body.position
```

### 7.3 å¥–åŠ±æœºåˆ¶ï¼š

æ¯åˆæˆä¸€ç§æ°´æœï¼ŒrewardåŠ ç›¸åº”çš„åˆ†æ•°


| æ°´æœ | åˆ†æ•° |
| -------- | -------- |
| æ¨±æ¡ƒ     | 2     |
| æ©˜å­     | 3     |
| ...     | ...     |
| è¥¿ç“œ     | 10     |
| å¤§è¥¿ç“œ     | 100     |

```python
if i < 11:
	self.last_score = self.score
	self.score += i
elif i == 11:
	self.last_score = self.score
	self.score += 100
```


### 7.4 æƒ©ç½šæœºåˆ¶:

å¦‚æœä¸€æ¬¡actionå 1sï¼ˆå³æ–°æ—§æ°´æœç”Ÿæˆé—´éš”ï¼‰æ²¡æœ‰æˆåŠŸåˆæˆæ°´æœï¼Œåˆ™rewardå‡å»æ”¾ä¸‹æ°´æœçš„åˆ†æ•°

```python
_, reward, _ = self.next_frame(action=action)
for _ in range(int(self.create_time * self.FPS)):
	_, nreward, _ = self.next_frame(action=None, generate=False)
	reward += nreward
	if reward == 0:
		reward = -i
```

### 7.5 è¾“å…¥ç‰¹å¾ï¼š

ä¹‹å‰çš„ç‰ˆæœ¬(https://aistudio.baidu.com/aistudio/projectdetail/1540300)è¾“å…¥ç‰¹å¾ä¸ºæ¸¸æˆæˆªå›¾ï¼Œé‡‡ç”¨ResNetæå–ç‰¹å¾

ä½†æ˜¯ç›´æ¥åŸå›¾è¾“å…¥ä½¿å¾—æ¨¡å‹å¾ˆéš¾å­¦ä¹ åˆ°æœ‰æ•ˆçš„ç‰¹å¾

å› æ­¤æ–°ç‰ˆæœ¬ä½¿ç”¨pygameæ¥å£è·å–å½“å‰çŠ¶æ€

```python
def get_feature(self, N_class=12, Keep=15):
        # ç‰¹å¾å·¥ç¨‹
        c_t = self.i
        # è‡ªèº«ç±»åˆ«
        feature_t = np.zeros((1, N_class + 1), dtype=np.float)
        feature_t[0, c_t] = 1.
        feature_t[0, 0] = 0.5
        feature_p = np.zeros((Keep, N_class + 1), dtype=np.float)
        Xcs = []
        Ycs = []
        Ts = []
        for i, ball in enumerate(self.balls):
            if ball:
                x = int(ball.body.position[0])
                y = int(ball.body.position[1])
                t = self.fruits[i].type
                Xcs.append(x/self.WIDTH)
                Ycs.append(y/self.HEIGHT)
                Ts.append(t)
        sorted_id = sorted_index(Ycs)
        for i, id_ in enumerate(sorted_id):
            if i == Keep:
                break
            feature_p[i, Ts[id_]] = 1.
            feature_p[i, 0] = Xcs[id_]
            feature_p[i, -1] = Ycs[id_]
            
        image = np.concatenate((feature_t, feature_p), axis=0)
        return image
```

æ³¨ï¼šN_class = æ°´æœç±»åˆ«æ•° + 1

#### feature_tï¼š
> ç”¨äºè¡¨ç¤ºå½“å‰æ‰‹ä¸­æ°´æœç±»åˆ«çš„ont-hotå‘é‡ï¼›

#### feature_pï¼š

> ç”¨äºè¡¨ç¤ºå½“å‰æ¸¸æˆçŠ¶æ€ï¼Œå¤§å°ä¸º(Keep, N_class + 1)

> Keep è¡¨ç¤ºåªå…³æ³¨å½“å‰ä½ç½®æœ€é«˜çš„ Keep ä¸ªæ°´æœ

> N_class - 1 æ˜¯æŸä¸ªæ°´æœç±»åˆ«çš„ont-hotå‘é‡ï¼Œ 0 ä½ç½®ä¸º x åæ ‡ï¼Œ-1 ä½ç½®ä¸º y åæ ‡ï¼ˆå½’ä¸€åŒ–ï¼‰

ç›®å‰ç”¨çš„å°±æ˜¯è¿™äº›ä¸ªç‰¹å¾ã€‚ã€‚ã€‚

# å¼ è€æ¿æ¥äº†hahhï¼Œè€æ¿å¤§æ°”ï¼ˆæ»‘ç¨½.jpgï¼‰

## 8. æˆ‘çš„å…¬ä¼—å·

![](https://ai-studio-static-online.cdn.bcebos.com/581bcc5e56594107859b2a8ccebba0e9938d10759d8242e689ae64680ab94150)

